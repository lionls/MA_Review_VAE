\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax 
\babel@toc {ngerman}{}
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {1}{\ignorespaces Transformer Encoder (links) und Transformer Decoder (rechts)}}{6}{figure.1}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {2}{\ignorespaces VAE Modellarchitektur von OPTIMUS mit BERT als Encoder und GPT-2 als Decoder}}{9}{figure.2}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {3}{\ignorespaces Methoden um den Latentvector in GPT-2 zu injizieren}}{9}{figure.3}%
