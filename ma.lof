\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax 
\babel@toc {ngerman}{}
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {1}{\ignorespaces Transformer Encoder (links) und Transformer Decoder (rechts)}}{6}{figure.1}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {2}{\ignorespaces VAE Modellarchitektur}}{8}{figure.2}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {3}{\ignorespaces VAE Modellarchitektur von OPTIMUS mit BERT als Encoder und GPT-2 als Decoder}}{10}{figure.3}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {4}{\ignorespaces Methoden um den Latentvector in GPT-2 zu injizieren}}{11}{figure.4}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {5}{\ignorespaces Latentraum $Z$ mit den entsprechenden generierten Bewertungen $X$.}}{15}{figure.5}%
