\section{Einleitung}\raggedbottom
Das automatisierte Zusammenfassen von Dokumenten ist im Bereich des Natural Language Processing (NLP) eine große Herausforderung.
Natural Language Processing ist ein Unterbereich der künstlichen Intelligenz, der sich mit dem maschinellen Verarbeiten von natürlicher Sprache auseinandersetzt. 
Aufgrund von neuen Methoden und immer größeren, komplexeren Netzwerken lassen sich hervorragende Ergebnisse in diversen NLP Aufgabenbereichen erzielen. 
Insbesondere lassen sich große, kostspielig vortrainierte Modelle mit vielen Parametern durch Transfer Learning in diversen speziellen Aufgabenbereichen einsetzen. 

Generative Pre-trained Transformers 2 (GPT-2) erzielte unter Verwendung von Transformern großartige Ergebnisse bei der Textgenerierung, unter anderem auch bei der abstraktiven Zusammenfassung von Texten. 
Die abstraktive Textzusammenfassung bezeichnet das Zusammenfassen von Texten zu einem kurzen, präzisen Text. 

\subsection{Motivation}
Im Bereich des E-Commerce und von Online-Vergleichsportalen entstehen eine große Anzahl an Rezensionen zu unterschiedlichen Produkten, Dienstleistungen und Anbietern.
Es ist für den Anwender eine Herausforderung sich einen repräsentativen Überblick über die einzelnen Rezensionen zu verschaffen. 
Ein gängiges Bewertungssystem ist das Berechnen des arithmetischen Mittels über die einzelnen Scores aller Bewertungen. 
Ein zusammengefasster Bewertungsscore ist nicht ausreichend repräsentativ für viele Produkte und stellt nicht spezifische Funktionen der einzelnen Produkte dar.
Hier bietet es sich an, ein Produkt durch eine generierte Rezension zu repräsentieren, die die anderen Rezensionen inkludiert und auf die verschiedenen wichtigen Aspekte der Produkte eingeht.
So kann ein Anwender sich zeitsparend einen ersten groben Überblick über bestimmte Produkte verschaffen.

\subsection{Ziel und Aufbau der Arbeit}
Ziel dieser Masterarbeit ist das unbeaufsichtigte abstraktive Zusammenfassen von mehreren Produktrezensionen mittels Variational Autoencodern zu einer repräsentativen Rezension.
Die generierten Zusammenfassungen sollten möglichst viele Aspekte der ursprünglichen Rezension aufgreifen, konsistent in ihrem Inhalt sein und einen guten Gesamtüberblick über die Rezensionen zu einem Produkt oder Dienstleistungen (zum Beispiel Restaurants) geben.
Als Datengrundlage gelten hier der Amazon-Review- und Yelp-Review-Datensatz, die zu Produkten und Dienstleistungen mehrere unterschiedliche diverse Rezensionen liefern.

Zunächst werden in Kapitel \ref{grundlagen} die verwendeten Grundlagen zu Textzusammenfassung, Deep Learning, Word Embeddings und Sequence To Sequence Modellen erläutert.
Anschließend werden in Kapitel \ref{transformer} die Transformermodelle Bidirectional Encoder Representations from Transformers (BERT) und GPT-2 erklärt, wobei detailliert auf die besonderen Merkmale wie Transformer Architekturen und Attention Layer eingegangen wird.
Der Attention Layer Aufbau ist insbesondere wichtig für die spätere Injektion der adaptierten Latentvektoren.

In Kapitel \ref{vae} wird der Aufbau und die mathematischen Details von Variational Autoencodern vermittelt.
Hier werden die zwei unterschiedlichen Variational Autoencoder Optimus und \textsc{BiMeanVAE} hergeleitet, die im weiteren Verlauf die Zusammenfassungen generieren.
Optimus basiert auf den beiden bekannten Sprachmodellen BERT und GPT-2. 
\textsc{BiMeanVAE} besteht aus einem bidirektionalem Long Short Term Memory (LSTM)-Encoder und einem LSTM-Decoder.

Im Anschluss werden in Kapitel \ref{dataset} die beiden Datensätze Amazon und Yelp vorgestellt, wobei die unterschiedlichen Aspekte und Herausforderungen der Datensätze dargestellt werden.

Danach werden in Kapitel \ref{multisum} mehrere aktuelle Verfahren zur Multi-Review Summarization vorgestellt. 
Convex Aggregation for Opinion Summarization (COOP) ist eins dieser Verfahren, welches unterschiedliche Kombinationen von einzelnen Latentvektoren untersucht.

In Kapitel \ref{pplm_chap} wird das vorgestellte COOP Verfahren durch ein Attributmodell weiter verbessert. 
Hierzu wird bei den Variational Autoencodern der Generationsprozess in Bezug auf ein Attributmodell optimiert.
Des Weiteren wird in diesem Kapitel eine neue Rankingfunktion zur Auswahl der Rezensionen eingeführt und die Hyperparameter für die Evaluation bestimmt.

Abschließend werden die konstruierten Modelle in Kapitel \ref{evalmetric} auf den Datensätzen evaluiert. Ebenfalls werden einzelne generierte Rezensionen untereinander verglichen.
In der abschließenden Zusammenfassung in Kapitel \ref{summary} wird ein Fazit und ein Ausblick auf weitere Forschungsgebiete gegeben.  




%Im NLP (\textbf{N}atural \textbf{L}anguage \textbf{P}rocessing) Bereich gibt es viele unterschiedliche Ansätze zur Textzusammenfassung von einzelnen Dokumenten.
%Da des Öfteren unterschiedliche Dokumente mit diversen Inhalten zu identischen Themen existieren, stellt sich die Frage wie diese unterschiedlichen Dokumente zu einem umfassenden Dokument zusammengefasst werden können.

%Ziel dieser Masterarbeit ist es mittels Variational Auto Encodern unterschiedliche Bewertungen zu Produkten und Restaurants abstraktiv zu einer allgemeingültigen Bewertung zusammenzufassen.




%Das Ziel dieser Arbeit ist mehrere Dokumente abstraktiv zu einem Dokument zusammenzufassen. Hierzu wird der Variational Auto Encoder OPTIMUS, der auf BERT und GPT-2 basiert verwendet. 

% Zunächst werden in Kapitel 2 die verwendeten Grundlagen zu Summarization, neuronalen Netzen, Deep Learning und den zugehörigen Subthemen erläutert. Ebenfalls wird auf die Einbettung von Wörtern eingegangen. Zur späteren Generierung von Question Answering Datensätzen werden in diesem Kapitel Abfragen mittels der Abfragesprache SPARQL an Knowledge Bases erklärt.

% Anschließend wird in Kapitel 3 die bidirektionale Transformer Architektur (BERT) erklärt, wobei detailliert auf die besonderen Merkmale wie Transformer Architekturen und Attention Layer eingegangen wird. Des Weiteren zeichnet sich BERT durch seine besonderen Pre-Trainingsmethoden, welche die Bidirektionalität des Modells ermöglichen, aus. 

% Im Anschluss werden in Kapitel 4, unter Verwendung von Python, Datensätze mit unterschiedlichen Vorgehensweisen generiert. Zum einen wird durch Abfragen auf Knowledge Bases ein Evaluationsdatensatz generiert, zum anderen wird durch maschinelle Übersetzung eines englischen Datensatzes ins Deutsche ein Trainings- und Evaluationsdatensatz erstellt. Es wird Bezug auf die besonderen Problematiken und Herausforderungen während der Entwicklung genommen. 

% Danach werden in Kapitel 5 zwei unterschiedlich vortrainierte Modelle mit den selbsterstellten Datensätzen und Kombinationen von anderen Datensätzen nachtrainiert. Insbesondere wird hier auf den Fine-Tuningsvorgang und die Wahl der unterschiedlichen Kombinationen der Datensätze eingegangen. 

% In Kapitel 6 werden die unterschiedlich trainierten Modelle evaluiert und untereinander verglichen. 
% In der abschließenden Zusammenfassung in Kapitel 7 wird ein Fazit und ein Ausblick auf weitere Forschungsgebiete gegeben.

% \pagebreak
% \subsection{Terminologie}

% \textbf{Adaptive Moment Estimation} (Adam) ist ein Optimierer für neuronale Netze.

% \textbf{Fine-Tuning} ist ein Trainingsprozess, der ein bereits vortrainiertes neuronales Netz an eine spezielle Aufgabenstellung anpasst und auf diese trainiert. Die bereits im Pre-Training gefundenen Parameter werden weiter angepasst.

% \textbf{Global Vectors for Word Representation} (GloVe) ein Verfahren für das Zuweisen von Vektorrepräsentationen zu Worten.

% \textbf{JavaScript Object Notation} (JSON) ist ein in Textform vorliegendes Datenformat.

% \textbf{Natural Language Processing} (NLP) beschreibt das maschinelle Verarbeiten und Analysieren von natürlichen Sprachen unter Verwendung von unterschiedlichen Techniken und Verfahren. 

% \textbf{Pre-Training} beschreibt den initialen Trainingsprozess eines neuronalen Netzes. Im Bereich des NLP wird Pre-Training verwendet, um ein allgemeines Sprachmodell zu trainieren, welches später auf individuelle Aufgabenstellungen nachtrainiert wird.

% \textbf{Tokenisierung} ist die Segmentierung eines Textes in eine Folge von Tokens. Die einzelnen Tokens sind Zeichenketten bestehend aus einem oder mehreren Zeichen.

\pagebreak
